#open train and validation setdef openfile(train, valid):    #train: training data    #valid: validation data        with open(train) as file:        train = []        read = csv.reader(file, delimiter = ',')        for line in read:            train.append(line)        train = np.array(train)             with open(valid) as file:        valid = []        read = csv.reader(file, delimiter = ',')        for line in read:            valid.append(line)        valid = np.array(valid)                return train, validdef split_data(data, n_output):    #data: each row having a label and the intput     output = data[:,0]    features = np.array(data[:,1:])    new_output = []            for num in output:        new_output.append(one_hot_vector(int(num),n_output))        return np.array(features), np.array(new_output), np.array(output).astype(float)def zero_initial(features, n_hidden, n_output):    #features: data to extract features from    #n_hidden: how many hidden neurons are there    #n_output: how many potential outcomes could there be        feature_length = np.shape(features)[1]    alpha = np.zeros((n_hidden, feature_length + 1))    beta = np.zeros((n_output, n_hidden + 1))    alpha_star = alpha[:,1:]    beta_star = beta[:,1:]        return np.array(alpha), np.array(beta), np.array(alpha_star), np.array(beta_star)def random_init(features, n_hidden, n_output):    #features: data to extract features from    #n_hidden: how many hidden neurons are there    #n_output: how many potential outcomes could there be        feature_length = np.shape(features)[1]    alpha_star = np.random.uniform(-.1, .1, (n_hidden, feature_length))    beta_star = np.random.uniform(-.1, .1, (n_output, n_hidden))        alpha_zeros = np.zeros((n_hidden, 1))    beta_zeros = np.zeros((n_output, 1))        alpha = np.hstack((alpha_zeros, alpha_star))    beta = np.hstack((beta_zeros, beta_star))        return np.array(alpha), np.array(beta), np.array(alpha_star), np.array(beta_star)def one_hot_vector(label, n_output):    vector = np.zeros(n_output)    vector[label] = 1    return vector#function for log loss errordef log_loss(vector, y):    loss = np.sum(-vector*np.transpose(np.log(y)))    return lossdef before_sigmoid(alpha, features):   a =  np.dot(alpha, features)   return adef softmax(beta, z):    b = np.dot(beta, z)    return b#feedfoward propogationdef feedfoward(output, features, alpha, beta, n_output): #feedfoward(training_output, training, alpha, beta,3)   features = np.array(features)[np.newaxis]   bias = [1]   features = np.concatenate((bias, features[0]))   features = np.transpose(features).astype(float)   a = before_sigmoid(alpha, features) #linear forward      z = [1] + [(1/(1+np.exp(-item))) for item in a] #sigmoid forward      b = softmax(beta, z) #linear forward      #calculate probabilities   denom = np.sum(np.array([np.exp(i) for i in b]))   y_hatsProbs = [(np.exp(i)/denom) for i in b] #soft max       y_hat = int(np.argmax(y_hatsProbs))  #find true y_hat based on finding the highest probability      vector = one_hot_vector(int(output), n_output) #one hot vector   J = log_loss(vector,y_hatsProbs)             return np.array(a), np.array(z), np.array(b), y_hatsProbs, y_hat, vector, J, featuresdef softmaxBackward(y_hatsProbs, vector, Gj):    dl_db = (y_hatsProbs - vector)*Gj    return dl_db    def linearbackward(b, z,dl_db, beta_star):        z_t = np.array(z)[np.newaxis]    rows = dl_db.shape[0]         dl_dBeta = dl_db.reshape(rows,1)*z_t    dl_dz = np.dot(dl_db.reshape(1,rows), beta_star).T        return dl_dBeta, dl_dz        def sigmoidbackward(a,z,dl_dz):        subtraction = z*(np.array([(1 - i) for i in z])[np.newaxis])    subtraction = subtraction[0][1:]        dl_daj = dl_dz.T * subtraction        return dl_dajdef linearbackward2(features, a,dl_daj):        features = np.array(features)[np.newaxis]    features = features.astype(float)        dl_dalpha = dl_daj.T* features    return dl_dalphadef backpropogation(output, features, alpha, beta, a, z, b, y_hatsProbs, y_hat, vector, J,beta_star):        #update weights on beta     Gj = 1        dl_db = softmaxBackward(y_hatsProbs, vector, Gj)    dl_dBeta, dl_dz = linearbackward(b, z,dl_db, beta_star)    dl_daj = sigmoidbackward(a,z,dl_dz)    dl_dalpha = linearbackward2(features, a,dl_daj)            return dl_dalpha, dl_dBetadef sgd(output, features, n_output,learning_rate, epochs, init_flag, n_hidden, train, valid):        if init_flag == 1:        alpha,beta,alpha_star,beta_star = random_init(features,n_hidden,n_output)    elif init_flag == 2:        alpha,beta,alpha_star,beta_star = zero_initial(features,n_hidden,n_output)            train_features, train_new_output, train_output = split_data(train, n_output)    valid_features, valid_new_output, valid_output = split_data(valid, n_output)        count = 0    train_final = []    valid_final = []        #while at each epoch    while count < epochs:        #for each row and output, use GSD to find the final alpha                train_loss = []        valid_loss = []                for i in range(len(output)):                        label = output[i].astype(float)            feat = features[i].astype(float)                        a, z, b, y_hatsProbs, y_hat, vector, J, bias_feature = feedfoward(label, feat, alpha, beta, n_output)            dl_dalpha, dl_dBeta = backpropogation(label, bias_feature, alpha, beta, a, z, b, y_hatsProbs, y_hat, vector, J,beta_star)            alpha = alpha-learning_rate*dl_dalpha            beta = beta- learning_rate*dl_dBeta                    #alpha_star = alpha[:,1:]            beta_star = beta[:,1:]                                    for i in range(len(output)):                        label = train[i][0].astype(float)            feat = train[i][1:].astype(float)            a, z, b, y_hatsProbs, y_hat, vector, J, bias_feature = feedfoward(label, feat, alpha, beta, n_output)                        prob = -np.log(y_hatsProbs[int(label)])                              train_loss.append(prob)                    for i in range(len(valid_output)):            label = valid[i][0].astype(float)            feat = valid[i][1:].astype(float)            a, z, b, y_hatsProbs, y_hat, vector, J, bias_feature = feedfoward(label, feat, alpha, beta, n_output)                        prob = -np.log(y_hatsProbs[int(label)])                              valid_loss.append(prob)                     train_mean = np.mean(np.array(train_loss))        valid_mean = np.mean(np.array(valid_loss))                 train_final.append(train_mean)        valid_final.append(valid_mean)                print("Epoch:%d"%(count+1))        #print("Alpha")        #print(alpha)        print("Beta")        print(beta)                count += 1                return alpha, beta, train_final, valid_final            def predict(data, alpha, beta, n_output):        #features, new_ouput, outputs = split_data(data, n_output)       pred_labels = []        for row in data:         label = np.array(row[0]).astype(float)        feat = np.array(row[1:]).astype(float)            a, z, b, y_hatsProbs, y_hat, vector, J, features = feedfoward(label, feat, alpha, beta, n_output)                pred_labels.append(y_hat)        return pred_labels#calculate the error rate based on the identification and the true outputdef errorcal(true_ouputs, labels):    error = 0        for i in range(len(labels)):        if true_ouputs[i] == labels[i]:            error = error        else:            error += 1        #calculations    error = (error)/len(labels)                return error               def main():     '''     import numpy as np    import csv    with open("smallTrain.csv") as file:        train = []        read = csv.reader(file, delimiter = ',')        for line in read:            train.append(line)        train = np.array(train)            with open("smallValidation.csv") as file:        valid = []        read = csv.reader(file, delimiter = ',')        for line in read:            valid.append(line)        valid = np.array(valid)            output_label = [0,1,2,3,4,5,6,7,8,9]    n_output = len(output_label)            n_hidden = 1    epochs = 100    init_flag = 1    learning_rate = .01            '''    train, valid = openfile(trains, validation)        output_label = [0,1,2,3,4,5,6,7,8,9]    n_output = len(output_label)        n_hidden = hidden_units    epochs = num_epoch    init_flag = init    learning_rate = learn           features, new_output, output = split_data(train, n_output)    valid_features, valid_new_output, valid_output = split_data(valid, n_output)        final_alpha, final_beta, train_loss, valid_loss = sgd(output, features, n_output,learning_rate, epochs, init_flag, n_hidden, train, valid)    train_labels = predict(train, final_alpha, final_beta, n_output)    valid_labels = predict(valid, final_alpha, final_beta, n_output)        train_error = errorcal(train_labels, output)    valid_error = errorcal(valid_labels,valid_output)            '''    ###WRITTEN PORTION     ####2ai        train_average = np.zeros(5)    valid_average = np.zeros(5)    new_alpha, new_beta = sgd(output, features, n_output,.01, 100, 1, 1, train, valid)            train_data = np.zeros(9000)    valid_data = np.zeros(1000)        for i in range(len(train)):        label = train[i][0].astype(float)        feat = train[i][1:].astype(float)                    a, z, b, y_hatsProbs, y_hat, vector, J, bias_feature = feedfoward(label, feat, new_alpha, new_beta, n_output)                train_data[i] = -np.log(y_hatsProbs[int(label)])                training = np.mean(train_data)      train_average[4] = training                        for i in range(len(valid)):        label = valid[i][0].astype(float)        feat = valid[i][1:].astype(float)                    a, z, b, y_hatsProbs, y_hat, vector, J, bias_feature = feedfoward(label, feat, new_alpha, new_beta, n_output)                    valid_data[i] = -np.log(y_hatsProbs[int(label)])        validating = np.mean(valid_data)      valid_average[4] = validating        print(train_average)    print(valid_average)    '''                                   ####2bi        new_alpha, new_beta, train_loss_1, valid_loss_1 = sgd(output, features, n_output,.1, 100, 1, 50, train, valid)    new_alpha, new_beta, train_loss_100, valid_loss_100 = sgd(output, features, n_output,.01, 100, 1, 50, train, valid)    new_alpha, new_beta, train_loss_1000, valid_loss_1000 = sgd(output, features, n_output,.001, 100, 1, 50, train, valid)        train_loss_1 = np.array(train_loss_1).T    valid_loss_1 = np.array(valid_loss_1).T    train_loss_100 = np.array(train_loss_100).T    valid_loss_100 = np.array(valid_loss_100).T    train_loss_1000 = np.array(train_loss_1000).T    valid_loss_1000 = np.array(valid_loss_1000).T            graph_1 = np.column_stack((train_loss_1,valid_loss_1))    graph_100 = np.column_stack((train_loss_100,valid_loss_100))    graph_1000 = np.column_stack((train_loss_1000,valid_loss_1000))        import pandas as pd     pd.DataFrame(graph_1).to_csv("/Users/ssriskanda/Documents/Carnegie_Mellon/Second_Year/First_Semester/10601/hw5/graph1")    pd.DataFrame(graph_100).to_csv("/Users/ssriskanda/Documents/Carnegie_Mellon/Second_Year/First_Semester/10601/hw5/graph100")    pd.DataFrame(graph_1000).to_csv("/Users/ssriskanda/Documents/Carnegie_Mellon/Second_Year/First_Semester/10601/hw5/graph1000")        sgd(output, features, n_output,.1, 1, 1, 4, train, valid)                    f = open(train_out, "w+")    for row in train_labels:        f.write("%d\n"%(row))    f.close()            g = open(validation_out, "w+")    for row in valid_labels:        g.write("%d\n"%(row))      g.close()        h = open(metrics,"w+" )    for i in range(epochs):        h.write("epoch=%d crossentropy(train): %f\n"%((int(i+1)),train_loss[i]))        h.write("epoch=%d crossentropy(valid): %f\n"%((int(i+1)),valid_loss[i]))    h.write("error(train): %f\n"%(train_error))    h.write("error(validation): %f"%(valid_error))    h.close()        if __name__ == '__main__':    import sys    import numpy as np    import csv        trains = sys.argv[1]    validation = sys.argv[2]    train_out = sys.argv[3]    validation_out = sys.argv[4]    metrics = sys.argv[5]    num_epoch = int(sys.argv[6])    hidden_units = int(sys.argv[7])    init = int(sys.argv[8])    learn = float(sys.argv[9])            main()    